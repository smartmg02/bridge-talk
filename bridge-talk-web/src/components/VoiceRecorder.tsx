'use client';

import React, { useRef, useState } from 'react';
import { supabase } from '@/lib/supabase';
import { v4 as uuidv4 } from 'uuid';

const VoiceRecorder = () => {
  const [recording, setRecording] = useState(false);
  const [audioURL, setAudioURL] = useState('');
  const [responseText, setResponseText] = useState('');
  const [manualInput, setManualInput] = useState('');
  const [highlightPoints, setHighlightPoints] = useState('');
  const [style, setStyle] = useState('warm'); // ä½¿ç”¨è€…é¸æ“‡çš„å›æ‡‰é¢¨æ ¼
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunks = useRef<Blob[]>([]);

  // ğŸ”µ åµæ¸¬è¼¸å…¥èªè¨€ï¼ˆç°¡å–®åˆ¤æ–·ï¼‰
  const detectLanguage = (text: string): 'zh' | 'en' | 'es' => {
    const zhRegex = /[\u4e00-\u9fff]/;
    const esRegex = /[Ã¡Ã©Ã­Ã³ÃºÃ±Ã¼Â¿Â¡]/i;
    
    if (zhRegex.test(text)) return 'zh';
    if (esRegex.test(text)) return 'es';
    return 'en'; // é è¨­è‹±æ–‡
  };

  const startRecording = async () => {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    mediaRecorderRef.current = new MediaRecorder(stream);
    audioChunks.current = [];

    mediaRecorderRef.current.ondataavailable = (e) => {
      audioChunks.current.push(e.data);
    };

    mediaRecorderRef.current.onstop = async () => {
      const audioBlob = new Blob(audioChunks.current, { type: 'audio/webm' });
      const url = URL.createObjectURL(audioBlob);
      setAudioURL(url);
      audioChunks.current = [];

      const formData = new FormData();
      formData.append('audio', audioBlob, 'voice.webm');

      const response = await fetch('/api/voice-to-text', {
        method: 'POST',
        body: formData
      });

      const result = await response.json();
      const userText = result.text;
      console.log('ğŸ—£ ä½¿ç”¨è€…å¿ƒè²(èªéŸ³)ï¼š', userText);

      await processUserText(userText, url);
    };

    mediaRecorderRef.current.start();
    setRecording(true);
  };

  const stopRecording = () => {
    mediaRecorderRef.current?.stop();
    setRecording(false);
  };

  const handleManualSubmit = async () => {
    if (!manualInput.trim()) {
      alert('è«‹è¼¸å…¥ä½ çš„å¿ƒè²ï¼ˆå®Œæ•´æè¿°ï¼‰å–”ï¼');
      return;
    }
    console.log('ğŸ“ ä½¿ç”¨è€…å¿ƒè²(æ–‡å­—)ï¼š', manualInput);
    console.log('âœ¨ ä½¿ç”¨è€… Highlight é‡é»ï¼š', highlightPoints);

    await processUserText(manualInput, null);

    setManualInput('');
    setHighlightPoints('');
  };

  const processUserText = async (fullText: string, audioUrl: string | null) => {
    const language = detectLanguage(fullText);

    const gptResponse = await fetch('/api/third-person-reply', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        full_description: fullText,
        highlight_points: highlightPoints,
        emotion_level: style === 'warm' ? 'mild' : style === 'rational' ? 'medium' : 'strong',
        language: language
      })
    });

    if (!gptResponse.ok) {
      const errorText = await gptResponse.text();
      console.error('ğŸ§¨ GPT API å›å‚³éŒ¯èª¤ï¼š', gptResponse.status, errorText);
      alert('âš ï¸ AI å›è¦†å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦');
      return;
    }

    const gptResult = await gptResponse.json();
    console.log('ğŸ¤– GPT å›æ‡‰ï¼š', gptResult.reply);

    setResponseText(gptResult.reply);

    try {
      const { error } = await supabase.from('records').insert([
        {
          id: uuidv4(),
          user_text: fullText,
          gpt_reply: gptResult.reply,
          audio_url: audioUrl
        }
      ]);
      if (error) {
        console.error('å„²å­˜åˆ° Supabase æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š', error.message);
      }
    } catch (err) {
      console.error('Supabase æ’å…¥è³‡æ–™å¤±æ•—ï¼š', err);
    }
  };

  return (
    <div className="p-4 border rounded shadow w-full max-w-md space-y-6">
      <h2 className="text-xl font-bold mb-4">èªéŸ³ï¼æ–‡å­—è¼¸å…¥</h2>

      {/* ğŸ”µ æ–‡å­—è¼¸å…¥å€ */}
      <div className="space-y-2">
        <textarea
          value={manualInput}
          onChange={(e) => setManualInput(e.target.value)}
          placeholder="è«‹è¼¸å…¥ä½ çš„å¿ƒè²ï¼ˆå®Œæ•´æè¿°ï¼‰..."
          className="w-full p-2 border rounded"
          rows={4}
        />
        <textarea
          value={highlightPoints}
          onChange={(e) => setHighlightPoints(e.target.value)}
          placeholder="è«‹è¼¸å…¥ç‰¹åˆ¥æƒ³å¼·èª¿çš„é‡é»ï¼ˆå»ºè­°1-3å€‹ï¼‰"
          className="w-full p-2 border rounded mt-2"
          rows={2}
        />
        <div className="mb-4">
          <label className="block mb-1 font-semibold">é¸æ“‡å›æ‡‰é¢¨æ ¼ï¼š</label>
          <select
  value={style}
  onChange={(e) => setStyle(e.target.value)}
  className="p-2 border rounded w-full"
>
  <option value="bestie">ğŸ‘¯ é–¨èœœ Bestie</option>
  <option value="peacemaker">ğŸ•Šï¸ å’Œäº‹ä½¬ Peacemaker</option>
  <option value="observer">ğŸ‘€ ä¸­ç«‹è§€å¯Ÿè€… Observer</option>
</select>

        </div>
        <button
          onClick={handleManualSubmit}
          className="px-4 py-2 bg-green-500 text-white rounded hover:bg-green-600"
        >
          âœ‰ï¸ é€å‡ºæ–‡å­—å¿ƒè²
        </button>
      </div>

      <hr />

      {/* ğŸ”µ èªéŸ³éŒ„éŸ³åŠŸèƒ½ */}
      <div className="space-y-2">
        <button
          onClick={recording ? stopRecording : startRecording}
          className={`px-4 py-2 rounded text-white ${recording ? 'bg-red-500' : 'bg-blue-500'}`}
        >
          {recording ? 'åœæ­¢éŒ„éŸ³' : 'é–‹å§‹éŒ„éŸ³'}
        </button>
        {audioURL && (
          <div className="mt-2">
            <audio src={audioURL} controls />
          </div>
        )}
      </div>

      {/* ğŸ”µ é¡¯ç¤º GPT å›æ‡‰ */}
      {responseText && (
        <div className="mt-6 p-4 bg-gray-100 rounded">
          <h3 className="font-semibold text-lg mb-2">ç¬¬ä¸‰äººè¨­å›æ‡‰ï¼š</h3>
          <p>{responseText}</p>
        </div>
      )}
    </div>
  );
};

export default VoiceRecorder;
